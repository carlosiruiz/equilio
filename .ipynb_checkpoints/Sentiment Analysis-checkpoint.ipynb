{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.208240Z",
     "start_time": "2021-09-03T19:01:47.889361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils.testing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-60ba3f2b8029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazypredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlazypredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lazypredict/Supervised.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils.testing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import lazypredict\n",
    "from sklearn import metrics\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.213922Z",
     "start_time": "2021-09-03T19:01:47.889Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "estimators = sklearn.utils.all_estimators(type_filter=None)\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.215941Z",
     "start_time": "2021-09-03T19:01:47.890Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Vault_diversity_reviews - Vault_diversity_reviews.csv',index_col='Unnamed: 0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.217850Z",
     "start_time": "2021-09-03T19:01:47.892Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df.drop(['company_rating','title', 'author','year'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.219739Z",
     "start_time": "2021-09-03T19:01:47.893Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Uses regex for removing punctuation and special characters.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n",
    "\n",
    "re_clean = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.221228Z",
     "start_time": "2021-09-03T19:01:47.894Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_review'] = pd.DataFrame(df.content.apply(re_clean))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.222519Z",
     "start_time": "2021-09-03T19:01:47.895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating own library of stop words\n",
    "add_stop_words = ['and', 'to', 'that','the','in','with','firm']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.224021Z",
     "start_time": "2021-09-03T19:01:47.897Z"
    }
   },
   "outputs": [],
   "source": [
    "#Count vectorizing all reviews. \n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range = (1,2))\n",
    "data_cv = cv.fit_transform(df.clean_review)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.225900Z",
     "start_time": "2021-09-03T19:01:47.898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create dictionaries with column names\n",
    "def top_words(column_name):\n",
    "    dictionary={}\n",
    "    brands=list(df['company'].unique())\n",
    "    for brand in brands:\n",
    "        tweet=''\n",
    "        for index in df[df['company']==brand].index:\n",
    "            tweet+=df[column_name][index]+' '\n",
    "        dictionary[brand]=tweet\n",
    "    return dictionary\n",
    "\n",
    "# We are going to change this to key: brand, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "def dict_to_dataframe(data_combined, column_name):\n",
    "    data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "    data_df.columns = [column_name]\n",
    "    data_df = data_df.sort_index()\n",
    "    return data_df\n",
    "\n",
    "# Find the top 30 words said by each company\n",
    "def top_30_words(tk_dataframe):\n",
    "    top_dict = {}\n",
    "    for word in tk_dataframe.columns:\n",
    "        top = tk_dataframe[word].sort_values(ascending=False).head(30)\n",
    "        top_dict[word] = list(zip(top.index, top.values))\n",
    "    return top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.227491Z",
     "start_time": "2021-09-03T19:01:47.899Z"
    }
   },
   "outputs": [],
   "source": [
    "#Combining all the company's reviews and cleanig the text.\n",
    "dict_review = top_words('clean_review')\n",
    "data_combined_review = {key: [combine_text(value)] for (key, value) in dict_review.items()}\n",
    "data_company = dict_to_dataframe(data_combined_review,'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.228984Z",
     "start_time": "2021-09-03T19:01:47.901Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.230351Z",
     "start_time": "2021-09-03T19:01:47.903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range = (2,2))\n",
    "\n",
    "# Fit and transform dataframe without data cleaning\n",
    "data_review_cv = cv.fit_transform(data_company.reviews)\n",
    "tk_reviews = pd.DataFrame(data_review_cv.toarray(), columns = cv.get_feature_names())\n",
    "tk_reviews.index = data_company.index\n",
    "\n",
    "# Transpose dataframes\n",
    "tk_reviews = tk_reviews.transpose()\n",
    "\n",
    "# Applying top_30_words function\n",
    "top_words_reviews = top_30_words(tk_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.231828Z",
     "start_time": "2021-09-03T19:01:47.904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the top 15 words\n",
    "def top_15_words(top_words_dict):\n",
    "    for brand, top_words in top_words_dict.items():\n",
    "        print(brand)\n",
    "        print(', '.join([word for word, count in top_words[0:14]])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.233606Z",
     "start_time": "2021-09-03T19:01:47.906Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Top Words Per company\\n')\n",
    "top_15_words(top_words_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.235378Z",
     "start_time": "2021-09-03T19:01:47.908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking out the word cloud for each company.\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "\n",
    "brands = list(top_words_reviews.keys())\n",
    "for index, brand in enumerate(tk_reviews.columns):\n",
    "    wc.generate(data_company.reviews[brands[0]])\n",
    "    plt.subplot(4,4,index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(brands[index] ,fontsize=22)\n",
    "    plt.savefig('mytable.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlobSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.237326Z",
     "start_time": "2021-09-03T19:01:47.909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.240011Z",
     "start_time": "2021-09-03T19:01:47.910Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subjectivity_review'] = df['clean_review'].apply(getSubjectivity)\n",
    "df['polarity_review'] = df['clean_review'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.242623Z",
     "start_time": "2021-09-03T19:01:47.912Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.244995Z",
     "start_time": "2021-09-03T19:01:47.913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a new column comparing the text blob results to the data set original analysis\n",
    "df['emotion_textblob'] = df['polarity_review'].apply(lambda x: 'Negative Emotion' if x < 0 else 'Neutral Emotion' if x == 0 else 'Positive emotion')\n",
    "df['target'] = df['polarity_review'].apply(lambda x: 0 if x < 0 else 1 if x == 0 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.246769Z",
     "start_time": "2021-09-03T19:01:47.914Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.248536Z",
     "start_time": "2021-09-03T19:01:47.916Z"
    }
   },
   "outputs": [],
   "source": [
    "#Finding the final sentiment of the company.\n",
    "def final_sentimet(column_name):\n",
    "    dictionary={}\n",
    "    brands=list(df['company'].unique())\n",
    "    for brand in brands:\n",
    "        polarity=[]\n",
    "        for index in df[df['company']==brand].index:\n",
    "            polarity.append(df[column_name][index])\n",
    "        dictionary[brand]=sum(polarity)/len(polarity)\n",
    "    sentiment= pd.DataFrame.from_dict(dictionary,orient=\"index\").reset_index()\n",
    "    sentiment.columns=['company','average_polarity']\n",
    "    sentiment['company_sentiment']=sentiment['average_polarity'].apply(lambda x: 'Negative Emotion' if x < 0 else 'Neutral Emotion' if x == 0 else 'Positive emotion')\n",
    "    return(sentiment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.250109Z",
     "start_time": "2021-09-03T19:01:47.917Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Compan's average sentiment polarity \n",
    "final_sentimet('polarity_review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.251947Z",
     "start_time": "2021-09-03T19:01:47.918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "       \n",
    "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
    "    print('Evaluation Metrics:')\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(y_true, y_pred)))\n",
    "    print('F1 Score: ' + str(metrics.f1_score(y_true, y_pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.253981Z",
     "start_time": "2021-09-03T19:01:47.919Z"
    }
   },
   "outputs": [],
   "source": [
    "X=df[\"clean_review\"]\n",
    "y=df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.255972Z",
     "start_time": "2021-09-03T19:01:47.920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train test split with TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_data_train = tfidf.fit_transform(X_train)\n",
    "tfidf_data_test = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.257929Z",
     "start_time": "2021-09-03T19:01:47.922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating Logistic Regression with Max Iter of 10000 so the model can reach convergence\n",
    "log_reg=LogisticRegression(max_iter=100000, random_state=40, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.259947Z",
     "start_time": "2021-09-03T19:01:47.923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting and predicting results.\n",
    "log_reg.fit(tfidf_data_train,y_train)\n",
    "\n",
    "log_pred=log_reg.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.261643Z",
     "start_time": "2021-09-03T19:01:47.924Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "\n",
    "\n",
    "mat = confusion_matrix(y_test,log_pred) \n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T19:01:51.263321Z",
     "start_time": "2021-09-03T19:01:47.925Z"
    }
   },
   "outputs": [],
   "source": [
    "#Final Evaluation.\n",
    "evaluation(y_test, log_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
